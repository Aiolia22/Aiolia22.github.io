---
title: "Principal Component Analysis with Python and sklearn"
data: 2020-11-01
tags: [PCA, Mario kart, data analyises]
header:
  image: "/images/chess.jpg"
excerpt: "Let‚Äôs play Mario Kart 64?"
---

# Introduction

Many of you have probably played Mario Kart 64, and each of you has a favorite character! I always played with Wario, which is certainly the best one üòä.
{: .text-justify}
In this game, we have 8 characters from the Nintendo franchise: Mario, Luigi, Princess Peach, Toad, Yoshi, Donkey Kong, Wario and Bowser. In general, we have the subdivision between light karts (Peach, Toad and Yoshi), medium karts (Mario and Luigi) and heavy karts (DK, Wario and Bowser), where it is common sense to say that the lighter ones have higher acceleration and the heavier ones have a higher speed; the medium ones are considered karts intermediaries.
{: .text-justify}
But, is it really that? Why Yoshi is from the light class and not from average class? Are Wario and Bowser the same? You probably never asked yourself these things before, but I asked myself and decided to investigate!
{: .text-justify}
To try to figure this out, we need to have some measurements for each kart. After a search on the internet, I found on [this site](https://gamefaqs.gamespot.com/n64/197860-mario-kart-64 / faqs / 27391) some features of each player. The features are:
{: .text-justify}
1. Time (s) required to reach 30 km/h;
2. Time (s) required to reach 50 km/h;
3. Maximum speed (km/h);
4. Time (s) required to reach maximum speed.

Unfortunately, I did not find information on the handling of each kart, which it would be important information for the analysis that we are going to do. The data is shown in Table 1.
{: .text-justify}

Table 1 ‚Äì Characteristics of each of the players in the Mario Kart 64 game.
{: .text-center}

| Player | TimeTo30kmph (s) |	TimeTo50kmph (s) |	TopSpeed (kmph) |	TimeToTopSpeed (kmph) |	WeightClass |
| :--- | :---: |	:---: |	:---: |	:---: |	:---: |
| Peach | 1.1 |	2.0 |	68 |	3.8 |	Light |
| Toad | 0.8 |	2.2 |	66 |	3.8 |	Light |
| Yoshi | 1.0 |	2.4 |	66 |	3.2 |	Light |
| Mario | 2.1 | 3.3	| 68	| 5.1	| Medium |
| Luigi	| 1.8	| 3.5	| 68	| 5.3	| Medium |
| Donkey Kong	| 2.0	| 2.6	| 70	| 4.2	| Heavy |
| Wario	| 2.0	| 2.7	| 70	| 4.0	| Heavy |
| Bowser	| 2.1	| 2.5	| 70	| 3.5	| Heavy |

When we look at the time to reach 30 km/h, we see a very clear difference between light and medium or heavy karts. But this characteristic is the same or is very confused between medium and heavy karts.
{: .text-justify}
As for the time to reach 50 km/h, we can see a difference between medium and heavy karts, where medium karts take longer to reach 50 km/h. Based on these data, we can say that light karts are the ones that accelerate faster at lower speeds.
When looking at the time needed to reach maximum speed, we see that the middle class is the slowest; however, the light and heavy classes overlapped. For example, look at Bowser that reaches maximum speed in less time than Toad and Peach, despite having a slower initial acceleration.
{: .text-justify}
Although it is possible to find some kind of pattern, as we saw through the maximum speed, in general, we don't have an obvious relationship between a kart being of the lightest class and accelerating faster and having a lower final speed, and it being heavier and then accelerating more slowly and have a higher final speed. In fact, we would even have something in that direction if it weren't for the middle class. But then, what characterizes a kart to be considered light, medium or heavy? Let's continue analyzing the data to try to find out.
{: .text-justify}
We could compare pairs of variables to try to find patterns more efficiently than just looking at numbers. An alternative is to several scatter plots (pair by pair) to try to find some kind of pattern, which can be seen in Figure 1.
{: .text-justify}

Figure 1 ‚Äì Scatter plot comparing each feature of the karts in Mario Kart 64.
{: .text-center}
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/scatter_plot.png" alt="cat" >

To try to find a pattern, we will need something more complex than an analysis of the numbers combined with a graphical analysis. I could use hierarchical group analysis, but I will use a statistical technique for reducing variables, which is principal component analysis (PCA). This technique consists of calculating new variables (called main components) based on the original variables.
{: .text-justify}
In fact, these main components are nothing more than linear combinations of the original variables. The great advantage of this method is that it has the potential to reduce the complexity of the data, making it easier to **identify patterns**, and maybe, the origin of the patterns. In other words, this technique has the potential to summarize all six graphs above in just 1 graph (depending on the data), which would certainly facilitate the interpretation of the results.
{: .text-justify}
We could use different programs/languages to do this, with R being the most obvious and simple choice (check [this incredible tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/)). We could also use STATISICA or even MATLAB, but they are very expensive programs, and you should never use a pirate program (really, since I developed one, I realized how easy it is to put some hidden lines of code to access data from users). For these and other reasons, I'm going to use Python, because the goal here is to use Python, because it's free and easy to use.
{: .text-justify}

# PCA in Python

Python provides at least two libraries to analyze the main components: sklearn and statsmodel. Each has its advantages and disadvantages. As far as I can tell, statsmodel is a little more versatile than sklearn. One of the advantages of statsmodel is that it allows you to use the NIPALS algorithm to perform the regression, which is a more usual method for experimental data than the SVG method, which is the only method available in sklearn. But sklearn seems to be used more in general, so I'm going to use sklearn.
{: .text-justify}
To do this study I am using Python 3.7, from the anaconda distribution and with the IDE Jupyter notebook. But you would not need to install anything at all to run this script on your computer (or cell phone). You could use [google colab](https://colab.research.google.com/), which is an online collaboration platform that allows you to use Python notebooks directly in your browser. Everything used here works on google colab without needing to install anything else. I will use Jupyter Notebook, as it is the way I am used to working with Python.
{: .text-justify}
With the Jupyter notebook open, we need to import some libraries that we will use when building the script:
{: .text-justify}
```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
from platform import python_version
print(python_version())
```
Output: 3.7.3

Next, we need to import the data. I created an Excel file with the data, and generated a ‚Äú.csv‚Äù file to be able to import the data in a Data Frame format. You can find a link to download this spreadsheet at the end of this text. To import the file, you have to place it in the same folder where the Jupyter notebook file is (the same goes for google colab, where the file must be on the virtual drive where the colab notebook is). Hence:
