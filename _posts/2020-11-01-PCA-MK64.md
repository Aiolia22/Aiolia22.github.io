---
title: "Principal Component Analysis with Python and sklearn"
data: 2020-11-01
tags: [PCA, Mario kart, data analyises]
header:
  image: "/images/chess.jpg"
excerpt: "Let‚Äôs play Mario Kart 64?"
---

# Introduction

Many of you have probably played Mario Kart 64, and each of you has a favorite character! I always played with Wario, which is certainly the best one üòä.
{: .text-justify}
In this game, we have 8 characters from the Nintendo franchise: Mario, Luigi, Princess Peach, Toad, Yoshi, Donkey Kong, Wario and Bowser. In general, we have the subdivision between light karts (Peach, Toad and Yoshi), medium karts (Mario and Luigi) and heavy karts (DK, Wario and Bowser), where it is common sense to say that the lighter ones have higher acceleration and the heavier ones have a higher speed; the medium ones are considered karts intermediaries.
{: .text-justify}
But, is it really that? Why Yoshi is from the light class and not from average class? Are Wario and Bowser the same? You probably never asked yourself these things before, but I asked myself and decided to investigate!
{: .text-justify}
To try to figure this out, we need to have some measurements for each kart. After a search on the internet, I found on [this site](https://gamefaqs.gamespot.com/n64/197860-mario-kart-64 / faqs / 27391) some features of each player. The features are:
{: .text-justify}
1. Time (s) required to reach 30 km/h;
2. Time (s) required to reach 50 km/h;
3. Maximum speed (km/h);
4. Time (s) required to reach maximum speed.

Unfortunately, I did not find information on the handling of each kart, which it would be important information for the analysis that we are going to do. The data is shown in Table 1.
{: .text-justify}

Table 1 ‚Äì Characteristics of each of the players in the Mario Kart 64 game.
{: .text-center}

| Player | TimeTo30kmph (s) |	TimeTo50kmph (s) |	TopSpeed (kmph) |	TimeToTopSpeed (kmph) |	WeightClass |
| :--- | :---: |	:---: |	:---: |	:---: |	:---: |
| Peach | 1.1 |	2.0 |	68 |	3.8 |	Light |
| Toad | 0.8 |	2.2 |	66 |	3.8 |	Light |
| Yoshi | 1.0 |	2.4 |	66 |	3.2 |	Light |
| Mario | 2.1 | 3.3	| 68	| 5.1	| Medium |
| Luigi	| 1.8	| 3.5	| 68	| 5.3	| Medium |
| Donkey Kong	| 2.0	| 2.6	| 70	| 4.2	| Heavy |
| Wario	| 2.0	| 2.7	| 70	| 4.0	| Heavy |
| Bowser	| 2.1	| 2.5	| 70	| 3.5	| Heavy |

When we look at the time to reach 30 km/h, we see a very clear difference between light and medium or heavy karts. But this characteristic is the same or is very confused between medium and heavy karts.
{: .text-justify}
As for the time to reach 50 km/h, we can see a difference between medium and heavy karts, where medium karts take longer to reach 50 km/h. Based on these data, we can say that light karts are the ones that accelerate faster at lower speeds.
When looking at the time needed to reach maximum speed, we see that the middle class is the slowest; however, the light and heavy classes overlapped. For example, look at Bowser that reaches maximum speed in less time than Toad and Peach, despite having a slower initial acceleration.
{: .text-justify}
Although it is possible to find some kind of pattern, as we saw through the maximum speed, in general, we don't have an obvious relationship between a kart being of the lightest class and accelerating faster and having a lower final speed, and it being heavier and then accelerating more slowly and have a higher final speed. In fact, we would even have something in that direction if it weren't for the middle class. But then, what characterizes a kart to be considered light, medium or heavy? Let's continue analyzing the data to try to find out.
{: .text-justify}
We could compare pairs of variables to try to find patterns more efficiently than just looking at numbers. An alternative is to several scatter plots (pair by pair) to try to find some kind of pattern, which can be seen in Figure 1.
{: .text-justify}

Figure 1 ‚Äì Scatter plot comparing each feature of the karts in Mario Kart 64.
{: .text-center}
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/scatter_plot.png" alt="cat" >

To try to find a pattern, we will need something more complex than an analysis of the numbers combined with a graphical analysis. I could use hierarchical group analysis, but I will use a statistical technique for reducing variables, which is principal component analysis (PCA). This technique consists of calculating new variables (called main components) based on the original variables.
{: .text-justify}
In fact, these main components are nothing more than linear combinations of the original variables. The great advantage of this method is that it has the potential to reduce the complexity of the data, making it easier to **identify patterns**, and maybe, the origin of the patterns. In other words, this technique has the potential to summarize all six graphs above in just 1 graph (depending on the data), which would certainly facilitate the interpretation of the results.
{: .text-justify}
We could use different programs/languages to do this, with R being the most obvious and simple choice (check [this incredible tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/)). We could also use STATISICA or even MATLAB, but they are very expensive programs, and you should never use a pirate program (really, since I developed one, I realized how easy it is to put some hidden lines of code to access data from users). For these and other reasons, I'm going to use Python, because the goal here is to use Python, because it's free and easy to use.
{: .text-justify}

# PCA in Python

Python provides at least two libraries to analyze the main components: sklearn and statsmodel. Each has its advantages and disadvantages. As far as I can tell, statsmodel is a little more versatile than sklearn. One of the advantages of statsmodel is that it allows you to use the NIPALS algorithm to perform the regression, which is a more usual method for experimental data than the SVG method, which is the only method available in sklearn. But sklearn seems to be used more in general, so I'm going to use sklearn.
{: .text-justify}
To do this study I am using Python 3.7, from the anaconda distribution and with the IDE Jupyter notebook. But you would not need to install anything at all to run this script on your computer (or cell phone). You could use [google colab](https://colab.research.google.com/), which is an online collaboration platform that allows you to use Python notebooks directly in your browser. Everything used here works on google colab without needing to install anything else. I will use Jupyter Notebook, as it is the way I am used to working with Python.
{: .text-justify}
With the Jupyter notebook open, we need to import some libraries that we will use when building the script:
{: .text-justify}
```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
from platform import python_version
print(python_version())
```
Output: 3.7.3

Next, we need to import the data. I created an Excel file with the data, and generated a ‚Äú.csv‚Äù file to be able to import the data in a Data Frame format. You can find a link to download this spreadsheet at the end of this text. To import the file, you have to place it in the same folder where the Jupyter notebook file is (the same goes for google colab, where the file must be on the virtual drive where the colab notebook is). Hence:
{: .text-justify}

```python
df = pd.read_csv('mario_kart_64.csv', delimiter=';')
df
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/table_df.png" alt="cat" >

The ‚Äúdelimiter‚Äù parameter is necessary in this case due to the way I created the ‚Äú.csv‚Äù file, which was made from an Excel spreadsheet. My computer uses commas as the standard decimal separator, so the csv file has a comma separating the decimal places instead of a point as a csv (comma-separated value) file should be.
{: .text-justify}
To clarify the nomenclature I adopted, understand the players (Mario, Luigi, Peach, Toad, Yoshi, Donkey Kong, Wario and Bowser) as individuals, and the characteristics of the karts (TimeTo30kmph_ (s), TimeTo50kmph_ (s), TopSpeed_ (kmph), TimeToTopSpeed_ (kmph)) as variables.
{: .text-justify}
To work with PCA, we have to use only numeric variables. Then the "Character" and "WeightClass" columns need to be removed. But for the construction of some graphs, we will need these columns. So, it is easier to create a new Data Frame (which I will call df_scaled), but with only the numeric data:
{: .text-justify}

```python
df_scaled = df.drop(['Personagem', 'WeightClass'], axis=1)
df_scaled
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/table_df_scaled.png" alt="cat" >

The ‚Äú.drop‚Äù method requires two arguments: the first is a list with the name of the columns to be removed; the second is the axis that we want to remove (as we want the column to be removed, we use axis = 1). The new Data Frame (‚Äúdf_scaled‚Äù, which will be changed shortly) has only the numerical data.
{: .text-justify}
Something extremely important for PCA analysis is to verify the size of the data. For this, it is interesting to check the Boxplot of the original data.
{: .text-justify}

```python
plt.figure(figsize=(16,8))
df_scaled.boxplot()
plt.grid(False)
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/boxplot.png" alt="cat" >

As we can see, with the exception of the maximum speed, the other variables have very close values. PCA analysis will always give higher importance to data with higher numerical value. So, if we use the data in its original form, the results will certainly be skewed to the maximum speed, since it has a numerical value much higher than the others. To check some basic statistics for the data, we can use the following method:
{: .text-justify}

```python
df_scaled.describe()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/describe.png" alt="cat" >

As we can see, the average of the TopSpeed variable is actually much higher than the others, which confirms a problem for the PCA.
{: .text-justify}
In addition, we have the problem that the units between the variables are not the same. We have time data (in seconds) that are related to the acceleration of each kart, and we have speed data (in km/h). This is also a serious problem for PCA analysis, as the PCA looks only at numbers, without considering units.
{: .text-justify}
For these reasons (and others not mentioned), it is common (and often advisable) to scale the data so that it starts to have an average equal to zero and a standard deviation equal to 1. Thus, all columns are in the same dimension, and the contribution for the main components it is similar. In this way, the data becomes dimensionless, and we no longer have problems with the different units. However, this is not always the right thing to do, it will depend on each case. For example, when we only have spectroscopic data, auto-scaling is probably not a good idea. In this case, it is essential.
{: .text-justify}
To transform the data for each variable with mean equal to zero and standard deviation equal to 1, we use Equation 1.
{: .text-justify}

\\[ x_{ij} =  \frac{\bar{x}_j - x_{ij}}{s_j}              (1)\\]


In Python, it is common to use the StandardScaler () method, which can be imported from the ‚Äúsklearn.preprocessing‚Äù library. However, this method uses the standard deviation for population data, not for sample data. When we have a lot of data, we would have no problem estimating the standard deviation as population. But in this case, and in the vast majority of cases with experimental data, we cannot consider the data as population-based. For this reason, StandardScaler () is not useful in this case.
{: .text-justify}
But this is not a problem, as we can do iteratively in Python itself in a very simple way. First, we need the number of variables and individuals:
{: .text-justify}
```python
n_individuals = df_scaled.shape[0]
n_variables = df_scaled.shape[1]
```
And we also need the column names:
```python
column_names = df_scaled.columns.values.tolist()
```

And now we need to calculate the mean and standard deviation for each column, and then iterate within each column, using Equation 1. And then, just do this for the other columns using a new for loop, within the first for loop. So, we need 2 for loops, which can be done as follows:
{: .text-justify}

```python
for j in range(n_variables):
    mean = df_scaled[column_names[j]].mean()
    std = df_scaled[column_names[j]].std(ddof=1)
    for i in range(n_individuals):
        df_scaled.loc[i,column_names[j]] = (df_scaled.loc[i,column_names[j]] - mean)/std
```

To make sure that the code above works, just use the Data Frames method ‚Äú.describe ()‚Äù again.
{: .text-justify}

```python
df_scaled.describe()d
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/describe2.png" alt="cat" >

As we can see, the average of each column is now equal to zero, and the standard deviation is equal to 1. So, the code is correct. We can generate a new Boxplot to check how the data looks:
{: .text-justify}

```python
plt.figure(figsize=(16,8))
df_scaled.boxplot()
plt.grid(False)
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/boxplot2.png" alt="cat" >

Now we can run the PCA, but first, we need to import the PCA from the sklearn library:
{: .text-justify}

```python
from sklearn.decomposition import PCA
```

Next, we create a PCA instance. It is at this point that we decide how many PCs will be calculated, and how the regression will be made. The number of PCs is determined by the parameter n_components. If this parameter is not passed, the number of PCs will be determined automatically as the lowest value between the number of variables or the number of individuals. In this case, we have 4 columns and 8 rows; and then, the maximum number of PCs is 4.
{: .text-justify}
At this point, we can also change the way the regression is done, but in general we have no reason to change. Unfortunately, sklearn does not have the option of using the NIPALS algorithm for regression, which is more suitable for experimental data, and uses the SVD method (the statsmodel library has both options implemented). Not that the SVD is bad, quite the contrary. It is also very efficient. As this is an exploratory analysis only, I will not change any parameters for the regression
{: .text-justify}

```python
pca = PCA()
```

Now let's look at the results. The first thing to do is check how much variance each PC can explain, which is done with the ‚Äúpca.explained_variance_‚Äù method. We can check the amount of variance that each PC explains as follows:
{: .text-justify}

```python
for i in range(len(pca.explained_variance_)):
    print("Variance explained by PC" + str(i + 1) + " = " + "{:.3f}".format(pca.explained_variance_[i]))
```

<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/var_explained.png" alt="cat" >

The first PC explains most of the entire data variance, the second explains a reasonable amount of the variance, and the other 2 explain the rest. We can generate a graph to better visualize the results, which is usually called ‚ÄúScree plot‚Äù:
{: .text-justify}


```python
# explainde variance plot
plt.figure(figsize=(16,8))
plt.plot(np.arange(1,len(pca.explained_variance_) + 1), pca.explained_variance_, marker='o')
plt.xticks(np.arange(1,len(pca.explained_variance_) + 1))
plt.xlabel("Principal Component")
plt.ylabel("Explained variance")
plt.title("Scree plot")
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/scree_plot.png" alt="cat" >

However, it is easier to compare the variance in percentage form. Therefore, we calculate the percentage of variance that each PC explains, using ‚Äúpca.explained_variance_ratio_‚Äù.
{: .text-justify}

```python
# explainde variance plot
# explained variance ratio
for i in range(len(pca.explained_variance_)):
    print("Variance explained (%) by PC" + str(i + 1) + " = " + "{:.2f}".format(pca.explained_variance_ratio_[i]*100))
```

<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/var_explained_ratio.png" alt="cat" >

PC1 explains 65% of all data variance; PC2 explains another 30%. So, with just 2 PCs, we were able to explain more than 95% of the entire data variance! This is very good, as it gives us good indications that we will be able to remove a lot of information when comparing only the first 2 PCs. It may be possible to reduce all 6 graphs in Figure 1 into just one graph.
{: .text-justify}
PCs are a linear combination of all the variables used. The first PC will always be the PC that contributes the most to the total variance, and it will always be in the direction where most of the data variance occurs. The second PC will always be perpendicular to the first PC. The third PC will always be perpendicular to the plane formed by PC1 and PC2. The fourth PC will always be perpendicular to the 3D system formed by PC1, PC2 and PC3, which would be a fourth dimension. And we will always have this sequence, so that the next PC will always be perpendicular to the composition of all previous PCs.
{: .text-justify}
We can check which weights were calculated for each PC. In this case, PC1 will be a combination of a number (ranging between -1 and 1, which is generally called weight) that accompanies the variable TimeTo30kmph_ (s), another number (between -1 and 1, which is another weight) that accompanies the variable TimeTo50kmph_ (s), another number (between -1 and 1, which is another weight) that accompanies the variable TopSpeed_ (kmph) and another number (between -1 and 1, which is another weight) that accompanies the TimeToTopSpeed_ (kmph) variable.
{: .text-justify}
The weight values can be accessed at ‚Äúpca.components_‚Äù. But to facilitate visualization, we can place the weights in a Data Frame. But before that, I will create a list with the name of each PC, just to facilitate the presentation of data in the Data Frame:
